{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "BASE = \"https://books.toscrape.com/\"\n",
    "def parse_book_block(article):\n",
    "    title = article.h3.a['title'].strip()\n",
    "    price = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "    availability = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "    star_classes = article.find(\"p\", class_=\"star-rating\")['class']\n",
    "    star_rating = [c for c in star_classes if c != \"star-rating\"][0]\n",
    "    return {\"title\": title, \"price\": price, \"availability\": availability, \"star_rating\": star_rating}\n",
    "books = []\n",
    "next_page = \"catalogue/page-1.html\"  \n",
    "resp = requests.get(BASE)\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "page_url = BASE\n",
    "while True:\n",
    "    print(f\"Fetching: {page_url}\")\n",
    "    resp = requests.get(page_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    articles = soup.select(\"article.product_pod\")\n",
    "    for art in articles:\n",
    "        books.append(parse_book_block(art))\n",
    "    next_li = soup.select_one(\"li.next > a\")\n",
    "    if not next_li:\n",
    "        break\n",
    "    next_href = next_li['href']\n",
    "    page_url = urljoin(page_url, next_href)\n",
    "    time.sleep(0.2) \n",
    "print(f\"Total books scraped: {len(books)}\")\n",
    "df = pd.DataFrame(books)\n",
    "df.to_csv(\"books.csv\", index=False)\n",
    "print(\"Saved books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "driver.get(url)\n",
    "time.sleep(1.5)  \n",
    "rows = driver.find_elements(By.CSS_SELECTOR, \"table.chart.full-width tbody tr\")\n",
    "movies = []\n",
    "for idx, row in enumerate(rows, start=1):\n",
    "    rank = idx\n",
    "    title_elem = row.find_element(By.CSS_SELECTOR, \"td.titleColumn a\")\n",
    "    title = title_elem.text.strip()\n",
    "    year_elem = row.find_element(By.CSS_SELECTOR, \"td.titleColumn span.secondaryInfo\")\n",
    "    year_text = year_elem.text.strip()  # e.g., \"(1994)\"\n",
    "    year = year_text.strip(\"()\")\n",
    "    rating_elem = row.find_element(By.CSS_SELECTOR, \"td.imdbRating strong\")\n",
    "    rating = rating_elem.text.strip()\n",
    "    movies.append({\"rank\": rank, \"title\": title, \"year\": year, \"imdb_rating\": rating})\n",
    "driver.quit()\n",
    "df = pd.DataFrame(movies)\n",
    "df.to_csv(\"imdb_top250.csv\", index=False)\n",
    "print(\"Saved imdb_top250.csv (rows: {})\".format(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import time\n",
    "BASE = \"https://www.timeanddate.com\"\n",
    "INDEX = \"https://www.timeanddate.com/weather/\"\n",
    "resp = requests.get(INDEX)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "anchors = soup.find_all(\"a\", href=True)\n",
    "city_links = []\n",
    "for a in anchors:\n",
    "    href = a['href']\n",
    "    if href.startswith(\"/weather/\") and href.count('/') >= 2:\n",
    "        parts = href.split('/')\n",
    "        if len(parts) >= 4 and parts[2] and parts[3]:\n",
    "            city_links.append(urljoin(BASE, href))\n",
    "seen = set()\n",
    "city_links_unique = []\n",
    "for link in city_links:\n",
    "    if link not in seen:\n",
    "        seen.add(link)\n",
    "        city_links_unique.append(link)\n",
    "print(f\"Found {len(city_links_unique)} candidate city links on index page\")\n",
    "N = 200\n",
    "city_links_unique = city_links_unique[:N]\n",
    "weather_rows = []\n",
    "for idx, link in enumerate(city_links_unique, start=1):\n",
    "    try:\n",
    "        print(f\"[{idx}/{len(city_links_unique)}] Fetching {link}\")\n",
    "        r = requests.get(link, timeout=8)\n",
    "        r.raise_for_status()\n",
    "        s = BeautifulSoup(r.text, \"lxml\")\n",
    "        h1 = s.find(\"h1\")\n",
    "        city_name = h1.text.strip() if h1 else link.split('/')[-1].replace('-', ' ').title()\n",
    "        qlook = s.find(id=\"qlook\")\n",
    "        if qlook:\n",
    "            temp_div = qlook.find(class_=\"h2\")\n",
    "            temp = temp_div.text.strip() if temp_div else None\n",
    "            cond_p = qlook.find(\"p\")\n",
    "            condition = cond_p.text.strip() if cond_p else None\n",
    "        else:       \n",
    "            temp = None\n",
    "            condition = None         \n",
    "            deg = s.find(lambda tag: tag.name in [\"div\", \"span\"] and \"Â°\" in tag.text)\n",
    "            temp = deg.text.strip() if deg else None         \n",
    "            meta_desc = s.find(\"meta\", {\"name\": \"description\"})\n",
    "            condition = meta_desc[\"content\"].strip() if meta_desc and \"Weather\" in meta_desc.get(\"content\",\"\") else None\n",
    "        weather_rows.append({\"city\": city_name, \"temperature\": temp, \"condition\": condition, \"url\": link})\n",
    "    except Exception as e:\n",
    "        print(\"  -> failed:\", e)\n",
    "    time.sleep(0.2)  \n",
    "df_weather = pd.DataFrame(weather_rows)\n",
    "df_weather.to_csv(\"weather.csv\", index=False)\n",
    "print(\"Saved weather.csv (rows: {})\".format(len(df_weather)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
